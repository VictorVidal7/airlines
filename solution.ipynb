{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.Analyze the distribution of the data provided in the dataset and draw any conclusions:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"dataset_SCL.csv\")\n",
    "\n",
    "# Analyze the distribution of the data\n",
    "print(df.describe())\n",
    "# Draw any conclusions based on the distribution of the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"dataset_SCL.csv\")\n",
    "\n",
    "# Statistical summary of the data\n",
    "print(df.describe())\n",
    "\n",
    "# Information about the data\n",
    "print(df.info())\n",
    "\n",
    "# Frequency of each unique value in a particular column\n",
    "print(df['TIPOVUELO'].value_counts())\n",
    "\n",
    "# Mean of a particular column based on a grouping column\n",
    "print(df.groupby('OPERA')['AÑO'].mean())\n",
    "\n",
    "# Use visualization libraries\n",
    "# Histogram\n",
    "df.hist(bins=50, figsize=(20, 15))\n",
    "plt.show()\n",
    "\n",
    "# Bar plot\n",
    "sns.countplot(x='DIA', data=df)\n",
    "plt.show()\n",
    "\n",
    "# Correlation matrix and heatmap\n",
    "corr_matrix = df.corr()\n",
    "sns.heatmap(corr_matrix, annot=True)\n",
    "plt.show()\n",
    "\n",
    "# Cross-tabulation\n",
    "pd.crosstab(df['SIGLADES'], df['TIPOVUELO'])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.Create additional columns and export them in a file called \"synthetic_features.csv\":\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "df['Fecha-I'] = pd.to_datetime(df['Fecha-I'])\n",
    "df['Fecha-O'] = pd.to_datetime(df['Fecha-O'])\n",
    "\n",
    "# Create the 'temporada_alta' column\n",
    "df['temporada_alta'] = np.where(((df['Fecha-I'].dt.month == 12) & (df['Fecha-I'].dt.day >= 15)) |\n",
    "                                ((df['Fecha-I'].dt.month == 3) & (df['Fecha-I'].dt.day <= 3)) |\n",
    "                                ((df['Fecha-I'].dt.month == 7) & (df['Fecha-I'].dt.day >= 15)) |\n",
    "                                ((df['Fecha-I'].dt.month == 7) & (df['Fecha-I'].dt.day <= 31)) |\n",
    "                                ((df['Fecha-I'].dt.month == 9) & (df['Fecha-I'].dt.day >= 11)) |\n",
    "                                ((df['Fecha-I'].dt.month == 9) & (df['Fecha-I'].dt.day <= 30)), 1, 0)\n",
    "\n",
    "# Create the 'dif_min' column\n",
    "df['dif_min'] = (df['Fecha-O'] - df['Fecha-I']).dt.total_seconds() / 60\n",
    "\n",
    "# Create the 'atraso_15' column\n",
    "df['atraso_15'] = np.where(df['dif_min'] > 15, 1, 0)\n",
    "\n",
    "# Create the 'periodo_dia' column\n",
    "df['periodo_dia'] = np.where((df['Fecha-I'].dt.hour >= 5) & (df['Fecha-I'].dt.hour < 12), 'morning',\n",
    "                             np.where((df['Fecha-I'].dt.hour >= 12) & (df['Fecha-I'].dt.hour < 19), 'afternoon', 'night'))\n",
    "\n",
    "# Export the new columns to a file\n",
    "df[['temporada_alta', 'dif_min', 'atraso_15', 'periodo_dia']].to_csv(\n",
    "    \"synthetic_features.csv\", index=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.Analyze the delay rate by destination, airline, month of the year, day of the week, season, and type of flight:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[\"Fecha-I\"] = pd.to_datetime(df[\"Fecha-I\"])\n",
    "\n",
    "# df[\"DIANOM\"] = df[\"Fecha-I\"].dt.weekday_name\n",
    "# df[\"DIA\"] = df[\"Fecha-I\"].dt.day\n",
    "# df[\"MES\"] = df[\"Fecha-I\"].dt.month\n",
    "# df[\"AÑO\"] = df[\"Fecha-I\"].dt.year\n",
    "\n",
    "\n",
    "# Analyze the delay rate by destination\n",
    "print(df.groupby('SIGLADES')['atraso_15'].mean())\n",
    "\n",
    "# Analyze the delay rate by airline. Print with a blank line after all the results.\n",
    "print(df.groupby('OPERA')['atraso_15'].mean())\n",
    "\n",
    "# Rate by month of the year\n",
    "print(df.groupby(df['Fecha-I'].dt.month)['atraso_15'].mean())\n",
    "\n",
    "# Analyze the delay rate by day of the week\n",
    "print(df.groupby(df['Fecha-I'].dt.day)['atraso_15'].mean())\n",
    "\n",
    "# Analyze the delay rate by season\n",
    "print(df.groupby('temporada_alta')['atraso_15'].mean())\n",
    "\n",
    "# Analyze the delay rate by type of flight\n",
    "print(df.groupby('TIPOVUELO')['atraso_15'].mean())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 'DIANOM': The day of the week can impact the likelihood of delays, as certain days\n",
    "\n",
    "may have higher traffic or more unpredictable weather.\n",
    "\n",
    "2. 'MES': Some months may have more extreme weather conditions or higher travel\n",
    "\n",
    "demand, which could impact the likelihood of delays.\n",
    "\n",
    "3. 'AÑO': The year may also be important, as economic and political factors can affect the\n",
    "\n",
    "number of flights and travel demand.\n",
    "\n",
    "4. 'OPERA': Different airlines may have different operational efficiencies and safety\n",
    "\n",
    "records, which can impact the likelihood of delays.\n",
    "\n",
    "5. 'SIGLAORI*: The city of origin may also be important, as certain cities may have more\n",
    "\n",
    "unpredictable weather or higher demand for flights.\n",
    "\n",
    "6. 'SIGLADES': The city of destination may also be important, as certain destinations may\n",
    "\n",
    "have more unpredictable weather or higher demand for flights.\n",
    "\n",
    "7. 'TIPOVUELO': the type of flight, international or national, can impact the likelihood of\n",
    "\n",
    "delays.\n",
    "\n",
    "8. 'DIA': Day of the month, certain days of the month may have higher traffic.\n",
    "\n",
    "9. 'temporada _alta': High season, travel season may have more flights, more demand,\n",
    "\n",
    "and more chances of delays.\n",
    "\n",
    "10. 'dif_min': Time difference between scheduled and operated flights, more difference\n",
    "\n",
    "may have more chances of delays.\n",
    "\n",
    "11. 'periodo _dia': time of the flight, morning, evening or night flights may have different\n",
    "\n",
    "chances of delays."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To determine which variables are most important in predicting delays, we can use various machine learning techniques like linear or logistic regression, Random Forest, Decision Trees, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "'''ValueError: could not convert string to float: 'Martes'\n",
    "This error is caused by the fact that the LinearRegression model in scikit-learn only accepts numerical data, but some of the columns in your dataframe contain non-numerical values such as 'Martes' or 'Internacional'.\n",
    "To fix this error, you will need to convert the non-numerical values in these columns to numerical values. One way to do this is by using the pandas function 'get_dummies' which converts categorical variables into numerical variables.\n",
    "Here is an example of how you can use get_dummies to convert 'DIANOM', 'OPERA', 'SIGLAORI', 'SIGLADES' and 'TIPOVUELO' columns to numerical values:'''\n",
    "\n",
    "# Prepare the data\n",
    "X = df[['MES', 'AÑO', 'DIA', 'temporada_alta', 'dif_min', 'periodo_dia']]\n",
    "X = pd.get_dummies(df[['DIANOM', 'OPERA', 'SIGLAORI', 'SIGLADES', 'TIPOVUELO']], drop_first=True)\n",
    "y = df['atraso_15']\n",
    "\n",
    "# X = df[['DIANOM', 'MES', 'AÑO', 'OPERA', 'SIGLAORI', 'SIGLADES', 'TIPOVUELO', 'DIA', 'temporada_alta', 'dif_min', 'periodo_dia']]\n",
    "# y = df['atraso_15']\n",
    "\n",
    "'''In this code snippet, we first select the numerical columns of our dataframe and then use get_dummies function to convert the non-numerical columns to numerical columns. We pass the dataframe containing the non-numerical columns to the get_dummies function, and set the drop_first parameter to True to remove one of the dummy variables for each categorical feature and reduce multicollinearity.\n",
    "Also, you can use the LabelEncoder from sklearn.preprocessing to convert non-numerical values to numerical values, it works by assigning a unique integer to each unique value in a column.'''\n",
    "\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# le = LabelEncoder()\n",
    "# df['DIANOM'] = le.fit_transform(df['DIANOM'])\n",
    "# df['OPERA'] = le.fit_transform(df['OPERA'])\n",
    "# df['SIGLAORI'] = le.fit_transform(df['SIGLAORI'])\n",
    "# df['SIGLADES'] = le.fit_transform(df['SIGLADES'])\n",
    "# df['TIPOVUELO'] = le.fit_transform(df['TIPOVUELO'])\n",
    "\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the linear regression model\n",
    "reg = LinearRegression()\n",
    "reg.fit(X_train, y_train)\n",
    "\n",
    "# Get the feature importance coefficients\n",
    "coefs = reg.coef_\n",
    "\n",
    "# Create a dictionary to store the feature importance scores\n",
    "feature_importance = dict(zip(X.columns, coefs))\n",
    "\n",
    "# Print the feature importance scores\n",
    "print(feature_importance)\n",
    "\n",
    "# Predict the test set labels\n",
    "y_pred = reg.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Accuracy\" + str(reg.score(X_test, y_test)))\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The above code uses linear regression to predict the 'atraso_15' column using the other columns in the dataset, and it calculates the importance of each feature by the coefficient of each feature in the linear regression model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the model and draw conclusions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = reg.predict(X_test)\n",
    "\n",
    "# Get the MSE of the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error:\", mse)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### When assessing model performance, it is important to choose the right metrics depending on the problem type and the characteristics of the data.\n",
    "\n",
    "In the case of a classification problem, some common metrics are accuracy, precision, recall, F1-score, and AUC-ROC.\n",
    "\n",
    "Accuracy: measures the proportion of correct predictions among all predictions.\n",
    "Precision: measures the proportion of true positive predictions among all positive predictions.\n",
    "Recall: measures the proportion of true positive predictions among all actual positive instances.\n",
    "F1-score: is the harmonic mean of precision and recall, and it gives a balance between the two.\n",
    "AUC-ROC: measures the ability of the model to distinguish between positive and negative classes, it is a good metric to use when there is an imbalance in the classes.\n",
    "In the case of a regression problem, some common metrics are mean squared error, mean absolute error, R-squared, and explained variance.\n",
    "\n",
    "Mean squared error: measures the average of the squared differences between the predicted and actual values.\n",
    "Mean absolute error: measures the average of the absolute differences between the predicted and actual values.\n",
    "R-squared: measures the proportion of the variation in the target variable that is explained by the features.\n",
    "Explained variance: measures the proportion of the variation in the target variable that is explained by the features.\n",
    "Once you have evaluated the performance of the model using the appropriate metrics, you can pick the best trained model and evaluate the following:\n",
    "\n",
    "Influential variables: You can use feature importance scores obtained from the model, or the coefficients of the features obtained by using linear regression, or using the feature importance score obtained by using Random Forest, Decision Trees etc, to determine which variables were most influential in the prediction task.\n",
    "Performance improvement: You can try different techniques such as feature engineering, hyperparameter tuning, ensemble models, data preprocessing, and using more data to improve the performance of the model.\n",
    "It is important to keep in mind that in some cases, the best model can be an ensemble of multiple models, or the performance of a model may be improved by a combination of different techniques."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ae303d8a5e3f6e903698197fd99ce9a219e1886fb13ea5d96d925c9b6ed3ec51"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
